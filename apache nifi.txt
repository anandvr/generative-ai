aug 10th
Apache NiFi is composed of various components that work together to facilitate data flow, processing, and integration. Here are the fundamental components of Apache NiFi:

Processor:
Processors are the core components responsible for ingesting, processing, transforming, routing, and outputting data. They perform specific tasks on the data as it moves through the data flow. NiFi provides a wide range of built-in processors to handle various data operations.

Connection:
Connections define the flow of data between processors. They represent the links between output ports of one processor and input ports of another. Connections also include back pressure management to ensure controlled data flow between processors.

FlowFile:
A FlowFile is a fundamental unit of data in NiFi. It represents the data being ingested, processed, or routed through the system. Each FlowFile contains the actual content along with attributes that provide metadata about the data.

Processor Group:
A Processor Group is a collection of processors, connections, and other components grouped together for organizational and operational purposes. It allows you to encapsulate and manage related components as a single unit.

Controller Service:
Controller Services provide reusable components that offer functionality to processors and other parts of the data flow. They can be configured globally and used by multiple processors to share resources and configuration settings.

Template:
Templates allow you to save a snapshot of a portion of your data flow as a reusable template. Templates can be imported/exported to share and replicate data flow configurations.

Input and Output Ports:
Input Ports and Output Ports allow data to be ingested into and exported from a Processor Group. They provide a way to connect external systems or other NiFi instances.

Funnel:
Funnels are used to merge data from different connections into a single stream for processing.

Remote Process Group:
A Remote Process Group allows you to interact with processors running in another NiFi instance, either on the same or a different physical machine. This is useful for managing data flows across distributed environments.

Flow Controller:
The Flow Controller manages the overall data flow, including starting and stopping processors, scheduling tasks, and coordinating data movement.

Bulletin Board:
The Bulletin Board displays system-level alerts, notifications, and diagnostic information about processors and components.

Provenance Repository:
The Provenance Repository stores data lineage and provenance information, enabling you to track the history of data as it flows through NiFi.


// Get the Controller Service context
def context = session.getProcessContext()

// Get the Controller Service by its name
def controllerService = context.getControllerServiceLookup().getControllerService('YourControllerServiceName')

// Get the property value (assuming the property is named 'databaseName')
def databaseName = controllerService.getProperty('databaseName').getValue()

// Set the extracted value as an attribute on the flow file
flowFile = session.putAttribute(flowFile, 'db_name_extracted', databaseName)

// Transfer the flow file to the next processor
session.transfer(flowFile, REL_SUCCESS)



class GroovyProcessor implements Processor {


    def REL_SUCCESS = new Relationship.Builder().name("success").description("FlowFiles that were successfully processed").build();
    def ProcessorLog log

    @Override
    void initialize(ProcessorInitializationContext context) {
        log = context.getLogger()
    }

    @Override

    Set<Relationship> getRelationships() {
        return [REL_SUCCESS] as Set
    }

    @Override
    void onTrigger(ProcessContext context, ProcessSessionFactory sessionFactory) throws ProcessException {
        try {

            def session = sessionFactory.createSession()
            def flowFile = session.get()
            if (!flowFile) return
            def selectedColumns = ''
            flowFile = session.write(flowFile,
                    { inputStream, outputStream ->
                        String line
                        final BufferedReader inReader = new BufferedReader(new InputStreamReader(inputStream, 'UTF-8'))
                        line = inReader.readLine()
                        String[] header = line?.split(',')
                        selectedColumns = "${header[1]},${header[2]}"                  
                        while (line = inReader.readLine()) {
                            String[] cols = line.split(',')
                            outputStream.write("${cols[2].capitalize()} ${cols[3].capitalize()}\n".getBytes('UTF-8'))
                        }
                    } as StreamCallback)

     flowFile = session.putAttribute(flowFile, "selected.columns", selectedColumns)
     flowFile = session.putAttribute(flowFile, "filename", "split_cols_invoke.txt")
            // transfer
            session.transfer(flowFile, REL_SUCCESS)
            session.commit()
        }
        catch (e) {
            throw new ProcessException(e)
        }
    }

    @Override
    Collection<ValidationResult> validate(ValidationContext context) { return null }

    @Override
    PropertyDescriptor getPropertyDescriptor(String name) { return null }

    @Override

    void onPropertyModified(PropertyDescriptor descriptor, String oldValue, String newValue) { }

    @Override

    List<PropertyDescriptor> getPropertyDescriptors() { return null }

    @Override

    String getIdentifier() { return null }
}

processor = new GroovyProcessor()